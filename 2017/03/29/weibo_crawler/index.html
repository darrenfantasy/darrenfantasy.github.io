<!DOCTYPE html>
<html lang="default">
<head>
  <meta charset="utf-8">
  
  <title>Python实现微博爬虫 | darrenfantasy</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="想获取某人发的所有微博信息，发现新浪微博的API里存在局限性，不仅需要申请高级接口，而且还要用户授权才能获取他发的全部微博。既然没有接口，那么就自己写个爬虫吧！
先附上项目代码地址：微博爬虫源码
项目的执行需要安装 selenium,requests以及BeautifulSoup库，还需要chromeDriver来配合。
首先，我们要在浏览器里打开微博去分析获取某个人的微博都需要哪些参数，以及调用">
<meta property="og:type" content="article">
<meta property="og:title" content="Python实现微博爬虫">
<meta property="og:url" content="http://yoursite.com/2017/03/29/weibo_crawler/index.html">
<meta property="og:site_name" content="darrenfantasy">
<meta property="og:description" content="想获取某人发的所有微博信息，发现新浪微博的API里存在局限性，不仅需要申请高级接口，而且还要用户授权才能获取他发的全部微博。既然没有接口，那么就自己写个爬虫吧！
先附上项目代码地址：微博爬虫源码
项目的执行需要安装 selenium,requests以及BeautifulSoup库，还需要chromeDriver来配合。
首先，我们要在浏览器里打开微博去分析获取某个人的微博都需要哪些参数，以及调用">
<meta property="og:updated_time" content="2017-04-02T08:44:26.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Python实现微博爬虫">
<meta name="twitter:description" content="想获取某人发的所有微博信息，发现新浪微博的API里存在局限性，不仅需要申请高级接口，而且还要用户授权才能获取他发的全部微博。既然没有接口，那么就自己写个爬虫吧！
先附上项目代码地址：微博爬虫源码
项目的执行需要安装 selenium,requests以及BeautifulSoup库，还需要chromeDriver来配合。
首先，我们要在浏览器里打开微博去分析获取某个人的微博都需要哪些参数，以及调用">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/typing.css">
  <link rel="stylesheet" href="/css/algolia.css">
  <link rel="stylesheet" href="/css/default-skin/default-skin.css">
</head>


<a href="https://github.com/darrenfantasy"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/38ef81f8aca64bb9a64448d0d70f1308ef5341ab/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6461726b626c75655f3132313632312e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_darkblue_121621.png"></a>

  
    <body>
  
      <div id="container" class="container">
        <article id="post-weibo_crawler" class="article article-type-post" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav id="main-nav" class="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/album">Album</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
      <a href="#" class="popup-trigger">Search</a>
  </nav>
  <nav id="sub-nav">
    
  </nav>
</header>
<div class="site-search">
  <div class="algolia-popup popup">
    <div class="algolia-search">
      <div class="algolia-search-input-icon">
        <i class="fa fa-search"></i>
      </div>
      <div class="algolia-search-input" id="algolia-search-input"></div>
    </div>

    <div class="algolia-results">
      <div id="algolia-stats"></div>
      <div id="algolia-hits"></div>
      <div id="algolia-pagination" class="algolia-pagination"></div>
    </div>

    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
  </div>
</div>
  <hr/>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Python实现微博爬虫
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <p>想获取某人发的所有微博信息，发现新浪微博的API里存在局限性，不仅需要申请高级接口，而且还要用户授权才能获取他发的全部微博。既然没有接口，那么就自己写个爬虫吧！</p>
<p>先附上项目代码地址：<a href="https://github.com/darrenfantasy/image_crawler/tree/master/SinaWeibo" target="_blank">微博爬虫源码</a></p>
<p>项目的执行需要安装 selenium,requests以及BeautifulSoup库，还需要chromeDriver来配合。</p>
<p>首先，我们要在浏览器里打开微博去分析获取某个人的微博都需要哪些参数，以及调用了哪些请求。</p>
<p>分析得出结果如下：</p>
<ol>
<li><p>获取微博的请求都需要有一个cookie，并且cookie存在的有效时间还是比较长的。</p>
</li>
<li><p>登录微博多次会需要验证码，为了避免验证码的阻碍，尽量把cookie存起来，等cookie失效了再去模拟登录获取cookie。</p>
</li>
<li><p>微博的每一页可以分为3屏，首屏的接口与2，3屏接口不一致。下面4，5两点的接口用的是<strong>MRJ台湾官方</strong>的微博为例子。</p>
</li>
<li><p>每一页的首屏接口为：<a href="http://weibo.com/mrj168?is_all=1&amp;profile_ftype=1&amp;page=1" target="_blank" rel="external">http://weibo.com/mrj168?is_all=1&amp;profile_ftype=1&amp;page=1</a> page为第几页</p>
</li>
<li><p>每一页的2，3屏接口为：<a href="http://weibo.com/p/aj/v6/mblog/mbloglist?ajwvr=6&amp;domain=100505&amp;pagebar=0&amp;is_tag=0&amp;is_search=0&amp;pre_page=1&amp;profile_ftype=1&amp;id=1005051837498771&amp;script_uri=%2Fmrj168&amp;feed_type=0&amp;__rnd=1490768727000&amp;pl_name=Pl_Official_MyProfileFeed__22&amp;is_all=1&amp;domain_op=100505&amp;page=1" target="_blank" rel="external">http://weibo.com/p/aj/v6/mblog/mbloglist?ajwvr=6&amp;domain=100505&amp;pagebar=0&amp;is_tag=0&amp;is_search=0&amp;pre_page=1&amp;profile_ftype=1&amp;id=1005051837498771&amp;script_uri=%2Fmrj168&amp;feed_type=0&amp;__rnd=1490768727000&amp;pl_name=Pl_Official_MyProfileFeed__22&amp;is_all=1&amp;domain_op=100505&amp;page=1</a> 需要修改的参数为pagebar,第二屏和第三屏分别为0，1。以及pre_page和page均为第几页。rnd为当前时间戳，单位是毫秒。id为100505+“微博的ID”，script_uri为“/”+“个性域名”或者”/“+”/u/“+”微博的ID”</p>
<p>​</p>
</li>
</ol>
<p>通过以上的分析，把逻辑转换成代码。大致流程如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line">result = is_valid_cookie()</div><div class="line"><span class="keyword">print</span> result</div><div class="line"><span class="keyword">if</span> result == <span class="keyword">False</span>:</div><div class="line">	driver = webdriver.Chrome(<span class="string">"/Users/fantasy/Downloads/chromedriver"</span>)<span class="comment">#打开Chrome</span></div><div class="line">	driver.maximize_window()<span class="comment">#将浏览器最大化显示</span></div><div class="line">	driver.get(weibo_url)<span class="comment">#打开微博登录页面</span></div><div class="line">	time.sleep(<span class="number">10</span>)<span class="comment">#因为加载页面需要时间，所以这里延时10s来确保页面已加载完毕</span></div><div class="line">	cookie = login_weibo_get_cookies()</div><div class="line">	save_cookie(cookie)</div><div class="line">	save_cookie_update_timestamp(get_timestamp())</div><div class="line"><span class="keyword">else</span> :</div><div class="line">	cookie = get_cookie_from_txt()</div><div class="line"><span class="keyword">for</span> x <span class="keyword">in</span> xrange(<span class="number">1</span>,page_size+<span class="number">1</span>):</div><div class="line">	profile_html = get_object_top_weibo_by_person_site_name_and_cookie(person_site_name,cookie,x)</div><div class="line">	image_url_list = get_img_urls_form_html(profile_html)</div><div class="line">	write_image_urls(image_url_list)</div><div class="line">	<span class="keyword">for</span> y <span class="keyword">in</span> xrange(<span class="number">0</span>,<span class="number">2</span>):<span class="comment">#有两次下滑加载更多的操作</span></div><div class="line">		<span class="keyword">print</span> <span class="string">"pagebar:"</span>+str(y)</div><div class="line">		html = get_object_weibo_by_weibo_id_and_cookie(weibo_id,person_site_name,cookie,y,x)</div><div class="line">		image_url_list = get_img_urls_form_html(html)</div><div class="line">		write_image_urls(image_url_list)</div></pre></td></tr></table></figure>
<p>首先判断本地是否存在有效的Cookie，如果Cookie不存在或者过期了，那么使用webdriver去打开微博登录并获取Cookie，然后更新本地的Cookie和更新时间。如果Cookie有效，则直接读取本地的Cookie。</p>
<p>有了Cookie之后，我们就可以拿着Cookie去调用上面分析出的两个接口啦！</p>
<ol>
<li>通过个性域名和Cookie及页码去请求某一页的首屏。</li>
<li>通过微博ID和个性域名及页码和第几屏去获取某一页的第几屏。</li>
</ol>
<p>接口返回的内容并不是json，而是HTML格式的文本，所以需要我们自己去解析。这里我使用的是BeautifulSoup来分析HTML的元素的。</p>
<p>以每页首屏的接口为例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_object_top_weibo_by_person_site_name_and_cookie</span><span class="params">(person_site_name,cookie,page)</span>:</span><span class="comment">#每一页顶部微博</span></div><div class="line">	<span class="keyword">try</span>:</div><div class="line">		profile_url = weibo_url+person_site_name+<span class="string">"?"</span></div><div class="line">		headers[<span class="string">"Cookie"</span>] = cookie</div><div class="line">		profile_request_params[<span class="string">"page"</span>] = page</div><div class="line">		response = requests.get(profile_url,headers=headers,params=profile_request_params)</div><div class="line">		<span class="keyword">print</span> response.url</div><div class="line">		html = response.text</div><div class="line">		soup = BeautifulSoup(html,<span class="string">"html.parser"</span>)</div><div class="line">		script_list = soup.find_all(<span class="string">"script"</span>)</div><div class="line">		script_size = len(script_list)</div><div class="line">		<span class="keyword">print</span> <span class="string">"script_size:"</span>+str(script_size)</div><div class="line">		tag = <span class="number">0</span></div><div class="line">		<span class="keyword">for</span> x <span class="keyword">in</span> xrange(script_size):</div><div class="line">			<span class="keyword">if</span> <span class="string">"WB_feed WB_feed_v3 WB_feed_v4"</span> <span class="keyword">in</span> str(script_list[x]):</div><div class="line">				tag = x</div><div class="line">		<span class="keyword">print</span> <span class="string">"tag:"</span>+str(tag)</div><div class="line">		<span class="comment"># print script_list[script_size-1]</span></div><div class="line">		html_start = str(script_list[tag]).find(<span class="string">"&lt;div"</span>)</div><div class="line">		html_end = str(script_list[tag]).rfind(<span class="string">"div&gt;"</span>)</div><div class="line">		<span class="comment"># print str(script_list[tag])[html_start:html_end+4]</span></div><div class="line">		<span class="keyword">return</span> str(str(script_list[tag])[html_start:html_end+<span class="number">4</span>])</div><div class="line">	<span class="keyword">except</span> Exception, e:</div><div class="line">		<span class="keyword">print</span> e</div><div class="line">	<span class="keyword">finally</span>:</div><div class="line">		<span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>接口返回的数据包含了很多东西，有30多个<em>script</em>标签，分析发现微博的内容在一个包含“WB_feed WB_feed_v3 WB_feed_v4”内容的<em>script</em>标签中。所以我们找到这个<em>script</em>，然后去掉头尾的无用信息，就获取到了我们想要的内容所在的<em>script</em>里。</p>
<p>接下来就是解析数据了，这里我想获取的是所发的微博里面所包含的图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_img_urls_form_html</span><span class="params">(html)</span>:</span><span class="comment">#从返回的html格式的字符串中获取图片</span></div><div class="line">	<span class="keyword">try</span>:</div><div class="line">		image_url_list = []</div><div class="line">		result_html = html.replace(<span class="string">"\\"</span>,<span class="string">""</span>)</div><div class="line">		soup = BeautifulSoup(result_html,<span class="string">"html.parser"</span>)</div><div class="line">		div_list = soup.find_all(<span class="string">"div"</span>,<span class="string">'media_box'</span>)</div><div class="line">		<span class="keyword">print</span> <span class="string">"div_list:"</span>+str(len(div_list))</div><div class="line">		<span class="keyword">for</span> x <span class="keyword">in</span> xrange(len(div_list)):</div><div class="line">			image_list = div_list[x].find_all(<span class="string">"img"</span>)</div><div class="line">			<span class="keyword">for</span> y <span class="keyword">in</span> xrange(len(image_list)):</div><div class="line">				image_url = image_list[y].get(<span class="string">"src"</span>).replace(<span class="string">"\\"</span>,<span class="string">""</span>)</div><div class="line">				<span class="keyword">print</span> image_url</div><div class="line">				image_url_list.append(image_url.replace(<span class="string">"\""</span>,<span class="string">""</span>))			</div><div class="line">		<span class="keyword">return</span> image_url_list</div><div class="line">	<span class="keyword">except</span> Exception, e:</div><div class="line">		<span class="keyword">print</span> e</div><div class="line">	<span class="keyword">finally</span>:</div><div class="line">		<span class="keyword">pass</span></div></pre></td></tr></table></figure>
<p>直接使用find_all来获取所有图片，再用get(“src”)来获取图片的url。</p>
<p>然后我的微博爬虫就这样实现了，好像也不难的样子。。。但是面对微博接口返回的一大堆数据，需要耐心去分析。</p>

      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2017/03/29/weibo_crawler/" class="article-date">
  <time datetime="2017-03-29T05:44:58.000Z" itemprop="datePublished">2017-03-29</time>
</a>

        </li>
        
        
          <li>
            <span class="label">Tag:</span>
            
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/crawler/">crawler</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/python/">python</a></li></ul>


          </li>
        
        <hr/>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2017/04/13/graphic_http/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          读《图解HTTP》笔记
        
      </div>
    </a>
  
  
    <a href="/2017/03/24/the_solution_to_get_cookie_from_js/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Python＋(selenium-webdriver)破解JS加密的Cookie</div>
    </a>
  
</nav>


  
</article>


  <section id="comments" class="comments">
  <div id="uyan_frame"></div>
<script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid=2128029"></script>
  </section>



      </div>
      
    <footer id="footer" class="post-footer footer">
      <hr/>
      <div id="footerContent" class="footer-content">
        <p>stay hungry , stay foolish .</p>


  
    <span id="busuanzi_container_page_pv">
         <i class="fa fa-flag"></i>    你是第<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>个阅读本文的小伙伴
    </span>
  
      </div>
  </footer>

      

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


  <script src="http://cdn.bootcss.com/instantsearch.js/1.5.1/instantsearch.js"></script>
  <script src="/js/algolia.js"></script>
  <script src="/js/photoswipe.js"></script>
  <script src="/js/photoswipe-ui-default.js"></script>

<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->







<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>
      <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
    </div>
  </body>
</html>
